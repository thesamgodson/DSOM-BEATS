{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSOM-BEATS: Usage Tutorial and Visualization Guide\n",
    "\n",
    "This notebook provides a complete walkthrough of how to use the DSOM-BEATS model, including:\n",
    "\n",
    "1.  **Configuration**: Loading and understanding the model configuration.\n",
    "2.  **Data Preprocessing**: Loading and splitting a benchmark dataset.\n",
    "3.  **Model Initialization**: Setting up the `DSOM_NBEATS` model.\n",
    "4.  **Training & Validation**: Running the training pipeline with a validation loop.\n",
    "5.  **Testing**: Evaluating the best model on the test set.\n",
    "6.  **Visualization**: Viewing the generated plots for SOM topology and regime analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's install the required packages and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Add project root to system path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from src.utils.config import load_config\n",
    "from src.models.nbeats import DSOM_NBEATS\n",
    "from src.pipelines.training import TrainingPipeline\n",
    "from src.data.preprocessing import DataPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration\n",
    "\n",
    "The model and training pipeline are configured using the `config.yml` file. Let's load it to see the parameters. You can change the benchmark dataset by editing the `data.name` field in `config.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('../config.yml')\n",
    "print(\"--- Data Configuration ---\")\n",
    "print(f\"Dataset Name: {config.data.name}\")\n",
    "print(f\"Splitting Ratios (Train/Val/Test): {config.data.splitting.train_ratio}/{config.data.splitting.val_ratio}/{config.data.splitting.test_ratio}\")\n",
    "\n",
    "print(\"--- Model Configuration ---\")\n",
    "print(f\"Lookback: {config.lookback}, Horizon: {config.horizon}\")\n",
    "print(f\"SOM Map Size: {config.som.map_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Benchmark Data\n",
    "\n",
    "We will now use the `DataPreprocessor` to automatically download, preprocess, and split the benchmark dataset specified in `config.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Process the dataset specified in the config\n",
    "datasets = preprocessor.process(config.data.name, config)\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(datasets['train'][0]).float(), torch.from_numpy(datasets['train'][1]).float()), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.from_numpy(datasets['val'][0]).float(), torch.from_numpy(datasets['val'][1]).float()), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(datasets['test'][0]).float(), torch.from_numpy(datasets['test'][1]).float()), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Loaded and preprocessed '{config.data.name}' dataset.\")\n",
    "print(f\"Number of samples (train/val/test): {len(train_loader.dataset)}/{len(val_loader.dataset)}/{len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Model and Training Pipeline\n",
    "\n",
    "Now, let's create an instance of the `DSOM_NBEATS` model and the `TrainingPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Determine number of features from the data\n",
    "# Input shape is (n_samples, lookback, n_features)\n",
    "n_features = datasets['train'][0].shape[2]\n",
    "print(f\"Detected {n_features} features.\")\n",
    "\n",
    "# Initialize model\n",
    "model = DSOM_NBEATS(config, n_features=n_features).to(device)\n",
    "\n",
    "# Initialize training pipeline\n",
    "pipeline = TrainingPipeline(model, config)\n",
    "\n",
    "print(\"Model and pipeline initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Training and Validation\n",
    "\n",
    "Let's train the model for a few epochs. After each epoch, the model is evaluated on the validation set, and the best-performing model checkpoint is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5 # Train for a small number of epochs for this demo\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    pipeline.train_epoch(train_loader, val_loader, epoch)\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the Best Model\n",
    "\n",
    "Now we load the best model (saved during training) and evaluate its performance on the unseen test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = pipeline.test(test_loader)\n",
    "\n",
    "print(\"--- Test Set Evaluation ---\")\n",
    "print(json.dumps(test_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Visualizations\n",
    "\n",
    "The training process should have created a directory (e.g., `visualizations/`) with the output plots for each epoch. Let's display the plots from the final epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "vis_dir = config.visualization.output_dir\n",
    "\n",
    "if not os.path.exists(vis_dir):\n",
    "    print(f\"Visualization directory '{vis_dir}' not found. Make sure training ran correctly.\")\n",
    "else:\n",
    "    # Display U-Matrix for the last epoch\n",
    "    u_matrix_path = os.path.join(vis_dir, f'som_u_matrix_epoch_{n_epochs}.png')\n",
    "    if os.path.exists(u_matrix_path):\n",
    "        print(\"--- SOM U-Matrix (Last Epoch) ---\")\n",
    "        display(Image(filename=u_matrix_path))\n",
    "    \n",
    "    # Display Component Planes for the last epoch\n",
    "    comp_planes_path = os.path.join(vis_dir, f'som_component_planes_epoch_{n_epochs}.png')\n",
    "    if os.path.exists(comp_planes_path):\n",
    "        print(\"\\n--- SOM Component Planes (Last Epoch) ---\")\n",
    "        display(Image(filename=comp_planes_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
